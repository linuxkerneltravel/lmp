# 2023.8.4 会议纪要

## 内存组

2023.08.04 内存小组
徐东：
1、操作系统真象还原完成13章学习与笔记编写，并出视频；
2、《操作系统 原理与实现》第10章，同步原语的实现学习与课后题，课后思考题。完成更深层次，更完整理论构建
3、阅读内存管理论文《用户态内存管理关键技术研究》
4、操作系统大赛编写ebpf程序替代ptrace功能
预计本月末尾，1，2完成，然后开启Linux2.4复现，探索内核源码，从内核源码尝试推进项目

贠可盈：
1.算法题4道（中等2道，线性表，简单两道，数组   算法题目需要多思考，有的题目需要一些不容易想到的技巧，还是要多积累）
2.操作系统真相还原开始第四章结束，开始第五章，正式进入内核学习
3.操作系统大赛小工具优化完善（1.出口函数跟踪 2.输出有效参数，无效的不输出 3.小工具转化成linux执行命令的形式  正在修改代码）
4.阅读论文 多变体执行安全防御技术研究综述_姚东

大部分时间还是给了操作系统大赛，毕竟马上要结束了

## 网络组

当前问题：网络方面缺少实际问题作为抓手，导致方向不明确

讨论结果：先看看别人都做了什么。车载领域比如国科础石，他们在网络方面都做了哪些工具，具体有哪些功能？我们能不能用eBPF做一套开源的替代方案？通用的网络领域比如nettrace，之前也看过但是没有深入进去继续研究，可以再研究研究他们做的点。

1.调研础石在网络方面的工具

![image-20230806220600062](https://raw.githubusercontent.com/nanshuaibo/picgo_image/master/image-20230806220600062.png)

https://github.com/alibaba/diagnose-tools/tree/master/SOURCE/module/net

2.继续研究nettrace

https://githu![image-20230806220623851](https://raw.githubusercontent.com/nanshuaibo/picgo_image/master/image-20230806220623851.png)b.com/OpenCloudOS/nettrace

3.关于网络性能优化，小组内其他成员对于XDP还不是很了解，计划由白宇宣同学梳理一份文档为大家讲解。

## CPU组

张子恒：

本周工作：
1. 开源之夏
2. 阅读论文《Linux内核中时间触发调度的评估》
3. 阅读《真相还原》

下周工作：
1. 开源之夏

2. 阅读一篇论文

3. 阅读《真相还原》

   

杨宁珂

本周工作：

1.将实验代码跑通（利用libbpf获取进程的CPU利率率，使用机器学习算法进行高负载下的异常检测，排除异常进进程之后，进行任务类型分类），目前只能进行异常检测，接下来就是训练各种数据集，然后将数据进行处理后，将异常进程拿出之后，进行任务分类

2.操作系统CPU这块继续学习，将真象还原和python里面相关部分都进行了进一步学习

下周工作：

1.将实验数据进行进一步处理，自我感觉数据里面还有问题，所以需要进一步处理一下，将数据准确性再提高

2.任务分类继续进行，争取下周将全部都进行完成。

利用libbpf进行数据提取：

```python
from numpy import asarray
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor
from matplotlib import pyplot
from sklearn.preprocessing import StandardScaler
import random
from sklearn.model_selection import GridSearchCV
# transform a timeseries dataset into a supervised learning dataset
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
n_vars = 1 #if type(data) is list else data.shape[1]
df = DataFrame(data)
# print(df)
cols = list()
# input sequence (t-n, ... t-1)
for i in range(n_in, 0, -1):
cols.append(df.shift(i))
# forecast sequence (t, t+1, ... t+n)
for i in range(0, n_out):
cols.append(df.shift(-i))
# put it all together
agg = concat(cols, axis=1)
# print(agg)
# drop rows with NaN values
if dropnan:
agg.dropna(inplace=True)
return agg.values
# split a univariate dataset into train/test sets
def train_test_split(data, n_test):
return data[:-n_test, :], data[-n_test:, :]
# fit an xgboost model and make a one step prediction
def xgboost_forecast(train, testX):
# transform list into array
train = asarray(train)
# split into input and output columns
trainX, trainy = train[:, :-1], train[:, -1]
# fit model
# cv_params = { 'n_estimators' : [200,250,300,350,400,450]}
# other_params = { 'learning_rate': 0.1, 'n_estimators': 400, 'max_depth':
5, 'min_child_weight': 1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree':
0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}
# model = XGBRegressor(**other_params)
# optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params,
scoring='r2', cv=5, verbose=1, n_jobs=4)
# optimized_GBM.fit(trainX, trainy)
# evalute_result = optimized_GBM.cv_results_
# print('每轮迭代运行结果:{0}' .format(evalute_result))
# print('参数的最佳取值：{0}' .format(optimized_GBM.best_params_))
# print('最佳模型得分:{0}' .format(optimized_GBM.best_score_))
model = XGBRegressor(objective='reg:squarederror', n_estimators=100)
model.fit(trainX, trainy)
# make a one-step prediction
yhat = model.predict(asarray([testX]))
return yhat[0]
# walk-forwardvalidation for univariate data
def walk_forward_validation(data, n_test):
predictions = list()
# split dataset
train, test = train_test_split(data, n_test)
# seed history with training dataset
history = [x for x in train]
# step over each time-step in the test set
for i in range(len(test)):
# split test row into input and output columns
testX, testy = test[i, :2], test[i, 2]
# fit model on history and make ap rediction
yhat = xgboost_forecast(history, testX)
# store forecast in list of predictions
predictions.append(yhat)
# add actual observation to history for the next loop
history.append(test[i])
# summarize progress
print('>expected=%.1f,predicted=%.1f' % (testy, yhat))
# estimate prediction error
error = mean_absolute_error(test[:, 1], predictions)
return error, test[:, 1], predictions
totalMSE = 0
series = read_csv('F:\python object\XGBoost\pre_522days_train_data.csv',
header=0,
index_col=0)
preprocess = StandardScaler()
values = preprocess.fit_transform(series)
print(values)
for country in range(0,2):
# load the dataset
print(values)
value = values[:, country]
# pyplot.plot(values)
# pyplot.show()
# transform the timeseries data into supervised learning
data = series_to_supervised(value, n_in=2)
# evaluate
mae, y, yhat = walk_forward_validation(data, 150
)
这段代码是一个使用XGBoost模型进行时间序列预测的示例。下面是对代码进行详细分析：
1. 首先，导入所需的库：numpy、pandas、sklearn等。
2. 定义了一个函数 series_to_supervised ，用于将时间序列数据转换为监督学习数据集。该函数会
将输入特征和输出标签按照指定的时间步长组合在一起，并返回转换后的数据集。
3. 定义了一个函数 train_test_split ，用于将数据集划分为训练集和测试集。
4. 定义了一个函数 xgboost_forecast ，用于训练XGBoost模型并进行单步预测。
5. 定义了一个函数 walk_forward_validation ，用于执行基于时间顺序的交叉验证。该函数会将训
练集和测试集分别传入模型进行训练和预测，并返回预测结果和评估指标（平均绝对误差）。
6. 加载时间序列数据，并使用 StandardScaler 对数据进行标准化处理。
7. 循环遍历每个进程的数据，并分别进行时间序列预测。
8. 在每个进程的预测结果中计算评估指标（平均绝对误差和均方误差），并累加均方误差到
totalMSE 变量中。
9. 最后，计算所有进程的均方误差的平均值并输出。
该代码使用XGBoost模型对时间序列数据进行预测，其中 series_to_supervised 函数用于将时间序列
数据转换为监督学习数据集， walk_forward_validation 函数用于执行交叉验证，并计算评估指标。
整体流程比较清晰，可以直接运行并查看结果。需要注意的是，该代码中使用的数据文件路径可能需要
根据实际情况进行修改。
1. 在循环中，首先从加载的数据中提取第一个进程的数据列（value）进行处理。
2. 使用 series_to_supervised 函数将原始数据转换为监督学习数据集。这里设置 n_in=2 ，表示每
个输入样本包含前两个时间步的数据，而 n_out=1 表示每个样本的输出标签是下一个时间步的数
据。转换后的数据将用于训练和测试模型。
3. 执行 walk_forward_validation 函数，对转换后的数据进行交叉验证。将训练集和测试集传入该
函数，它会根据输入的时间步顺序逐步进行模型训练和预测，并返回预测结果以及评估指标（平均
绝对误差）。
4. 在循环的每一次迭代中，预测结果存储在 predictions 列表中，并打印出每个测试样本的实际值
和预测值。
5. 针对每个进程的预测结果，计算平均绝对误差和均方误差，分别存储在 mae 和 mse 变量中。这些指
标是评估预测精度的常用指标，用于衡量实际值与预测值之间的差异。
# mae = mae + 5
mse = 0
for i in range(len(y)):
mse = mse + (y[i] - yhat[i]) ** 2
# print(mse)
mse = mse / 100
totalMSE = totalMSE + mse
print('MAE: %.3f' % mae) #平均绝对误差
print('MSE: %.3f' % mse) #均方误差
# for i in range(len(yhat)):
# randint_data = random.randint(-10,10)
# yhat[i] += randint_data
# plot expected vspreducted
pyplot.plot(y, label='Expected')
pyplot.plot(yhat, label='Predicted')
pyplot.legend()
pyplot.show()
# print("totalMSE", totalMSE)
totalMSE = totalMSE/3 #多少列数据 就除多少
print("totalMSE", totalMSE)
```

这段代码是一个使用XGBoost模型进行时间序列预测的示例。下面是对代码进行详细分析： 1. 首先，导入所需的库：numpy、pandas、sklearn等。 2. 定义了一个函数 series_to_supervised ，用于将时间序列数据转换为监督学习数据集。该函数会 将输入特征和输出标签按照指定的时间步长组合在一起，并返回转换后的数据集。 3. 定义了一个函数 train_test_split ，用于将数据集划分为训练集和测试集。 4. 定义了一个函数 xgboost_forecast ，用于训练XGBoost模型并进行单步预测。 5. 定义了一个函数 walk_forward_validation ，用于执行基于时间顺序的交叉验证。该函数会将训 练集和测试集分别传入模型进行训练和预测，并返回预测结果和评估指标（平均绝对误差）。 6. 加载时间序列数据，并使用 StandardScaler 对数据进行标准化处理。 7. 循环遍历每个进程的数据，并分别进行时间序列预测。 8. 在每个进程的预测结果中计算评估指标（平均绝对误差和均方误差），并累加均方误差到 totalMSE 变量中。 9. 最后，计算所有进程的均方误差的平均值并输出。

## RTOS组

南帅波：

- 真相还原目前阅读到第六章（在内核实现自己的打印函数）
- 完善鸿蒙移植考察文档
- 论文：Effects of Dynamic Isolation for Full Virtualized RTOS and GPOS Guests(动态隔离对全虚拟化RTOS和GPOS客户机的影响 2016) 
- 推进操作系统大赛


刘冰：

- 完善鸿蒙移植考察文档

- 阅读奔跑吧Linux内核

- 迭代开源之夏项目

  - 理解符号地址解析函数的实现

  - 适配低版本libbpf uprobe接口

- 刷力扣算法题